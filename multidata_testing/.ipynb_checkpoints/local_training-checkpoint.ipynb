{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 1)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "x_train shape: (60000, 28, 28), y_train shape: (60000,)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPooling2D\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import client_util\n",
    "import os\n",
    "import federated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clientDir(clientNum,cwd):\n",
    "    folderPath = \"client\" + str(clientNum)\n",
    "    folderDir = os.path.join(cwd,folderPath,'model_weights')\n",
    "    if not os.path.isdir(folderDir):\n",
    "        print('creating the model_weights folder')\n",
    "        os.makedirs(folderDir)\n",
    "    os.chdir(folderDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['cifar10','mnist','fashion_mnist']\n",
    "curr_dataset = datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare image parameters\n",
    "if curr_dataset=='cifar10':\n",
    "    NUM_CHANNELS = 3\n",
    "    img_size = 32\n",
    "else:\n",
    "    img_size = 28\n",
    "    NUM_CHANNELS = 1\n",
    "NUM_CLASSES=10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 1)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 10)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#get data    \n",
    "x_train_c,y_train_c,x_test_c,y_test_c = client_util.load_dataset(datasets[0])\n",
    "x_train_c,y_train_c,x_test_c,y_test_c = client_util.normalize_data(x_train_c,y_train_c,x_test_c,y_test_c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (12500, 32, 32, 3), y_train shape: (12500, 10)\n"
     ]
    }
   ],
   "source": [
    "#split data for respective clients\n",
    "clients = client_util.in_order_sampling(x_train_c,y_train_c,4)\n",
    "#test training with first client\n",
    "x_train = clients[0][0]\n",
    "y_train = clients[0][1]\n",
    "print(\"x_train shape: {}, y_train shape: {}\".format(x_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare placeholders\n",
    "x = tf.placeholder(tf.float32,shape=[None,img_size,img_size,NUM_CHANNELS])\n",
    "y_actual = tf.placeholder(tf.float32,shape=[None,NUM_CLASSES])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================Setting up Cifar10 model=================\n",
      "W1_shape: Tensor(\"conv_1_1/Shape:0\", shape=(4,), dtype=int32), b1_shape: Tensor(\"conv_1_1/Shape_1:0\", shape=(1,), dtype=int32)\n",
      "W2_shape: Tensor(\"conv_2_1/Shape:0\", shape=(4,), dtype=int32), b2_shape: Tensor(\"conv_2_1/Shape_1:0\", shape=(1,), dtype=int32)\n",
      "W_fc1_shape: Tensor(\"fc_1_1/Shape:0\", shape=(2,), dtype=int32), b_fc1_shape: Tensor(\"fc_1_1/Shape_1:0\", shape=(1,), dtype=int32)\n",
      "W_fc2_shape: Tensor(\"Shape_2:0\", shape=(2,), dtype=int32), b_fc2_shape: Tensor(\"Shape_3:0\", shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "y_pred = federated_model.model(x,y_actual,NUM_CHANNELS,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-b6b48ae6d977>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#setting up loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_actual,logits=y_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('acc',reuse = tf.AUTO_REUSE):\n",
    "   matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_actual,1))\n",
    "   acc = tf.reduce_mean(tf.cast(matches,tf.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    test_acc_list = []\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        #shuffle data \n",
    "        np.random.shuffle(idx)\n",
    "        x_train, y_train = x_train[idx],y_train[idx]\n",
    "        for start,end in zip(range(0,N,batch_size),range(batch_size,N,batch_size)):\n",
    "            sess.run(train,feed_dict = {x:x_train[start:end], y_actual:y_train[start:end],keep_prob: 0.5})\n",
    "        \n",
    "        \n",
    "        train_acc = acc.eval(feed_dict = {x:x_train[start:end], y_actual:y_train[start:end],keep_prob: 0.5})\n",
    "        train_acc_list.append(train_acc)\n",
    "        \n",
    "        train_loss = cross_entropy.eval(feed_dict = {x:x_train[start:end], y_actual:y_train[start:end],keep_prob: 0.5})\n",
    "        train_loss_list.append(train_loss)\n",
    "        \n",
    "        test_acc = acc.eval(feed_dict = {x:x_train[start:end], y_actual:y_train[start:end],keep_prob: 0.5})\n",
    "        test_acc_list.append(test_acc)\n",
    "        \n",
    "        if epoch%2==0:\n",
    "            print(\"Epoch: {}, Training accuracy: {}, Testing accuracy: {}, Training loss: {}\".format(epoch,train_acc,test_acc,train_loss))\n",
    "           \n",
    "    for var in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES):\n",
    "        weight = (sess.run([var]))[0].flatten().tolist()\n",
    "        filename = (str(var).split())[1].replace('/', '_')\n",
    "        filename = filename.replace(\"'\", \"\").replace(':0', '') + '.txt'\n",
    "        print(\"saving {}\".format(filename))\n",
    "        clientDir(1,cwd)\n",
    "        np.savetxt(str(filename), weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 1)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
      "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 10)\n",
      "x_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    datasets = ['cifar10','mnist','fashion_mnist']\n",
    "    curr_dataset = datasets[0]\n",
    "    \n",
    "    #declare image parameters\n",
    "    if curr_dataset=='cifar10':\n",
    "        NUM_CHANNELS = 3\n",
    "        img_size = 32\n",
    "    else:\n",
    "        img_size = 28\n",
    "        NUM_CHANNELS = 1\n",
    "    NUM_CLASSES=10   \n",
    "    \n",
    "    #get data    \n",
    "    x_train_c,y_train_c,x_test_c,y_test_c = client_util.load_dataset(datasets[0])\n",
    "    x_train_c,y_train_c,x_test_c,y_test_c = client_util.normalize_data(x_train_c,y_train_c,x_test_c,y_test_c )\n",
    "    \n",
    "    #split data for respective clients\n",
    "    clients = in_order_sampling(x_train,y_train,num_clients)\n",
    "    \n",
    "    #declare placeholders\n",
    "    x = tf.placeholder(tf.float32,shape=[None,img_size,img_size,NUM_CHANNELS])\n",
    "    y_actual = tf.placeholder(tf.float32,shape=[None,NUM_CLASSES])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    #setting up loss function\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_actual,logits=y_pred))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train = optimizer.minimize(cross_entropy)\n",
    "    \n",
    "    with tf.variable_scope('acc',reuse = tf.AUTO_REUSE):\n",
    "        matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_actual,1))\n",
    "        acc = tf.reduce_mean(tf.cast(matches,tf.float32)) \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "federated_learning",
   "language": "python",
   "name": "federated_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
